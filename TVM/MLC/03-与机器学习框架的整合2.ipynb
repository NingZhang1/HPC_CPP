{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tvm\n",
    "from tvm import relax\n",
    "from tvm.ir.module import IRModule\n",
    "from tvm.script import relax as R\n",
    "from tvm.script import tir as T\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import fx\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from tvm import te"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.weight = nn.Parameter(torch.randn(128, 128))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.matmul(x, self.weight)\n",
    "        x = torch.relu(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "opcode         name    target                                                     args         kwargs\n",
      "-------------  ------  ---------------------------------------------------------  -----------  --------\n",
      "placeholder    x       x                                                          ()           {}\n",
      "get_attr       weight  weight                                                     ()           {}\n",
      "call_function  matmul  <built-in method matmul of type object at 0x7f9e79f97760>  (x, weight)  {}\n",
      "call_function  relu    <built-in method relu of type object at 0x7f9e79f97760>    (matmul,)    {}\n",
      "output         output  output                                                     (relu,)      {}\n"
     ]
    }
   ],
   "source": [
    "model = MyModel()\n",
    "fx_module = fx.symbolic_trace(model)\n",
    "type(fx_module)\n",
    "\n",
    "fx_module.graph.print_tabular()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "大多数机器学习框架都带有计算图抽象，其中每个节点对应一个操作，边对应它们之间的依赖关系。 我们将采用 PyTorch 模型，获取 PyTorch 原生格式的计算图，并将其转换为 IRModule。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "让我们定义整体的翻译逻辑。 主要流程如下：\n",
    "\n",
    "1. 创建一个 node_map，将 fx.Node 映射到相应的 relax.Var，该 relax.Var 代表 IRModule 中的已翻译节点。\n",
    "\n",
    "2. 以拓扑顺序迭代 FX 图中的节点。\n",
    "\n",
    "3. 给定映射输入，获取节点的映射输出。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "named_modules =  {'': MyModel()}\n",
      "node =  x\n",
      "node =  weight\n",
      "node =  matmul\n",
      "node =  relu\n",
      "node =  output\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"highlight\" style=\"background: \"><pre style=\"line-height: 125%;\"><span></span><span style=\"color: #007979; font-style: italic\"># from tvm.script import ir as I</span>\n",
       "<span style=\"color: #007979; font-style: italic\"># from tvm.script import tir as T</span>\n",
       "<span style=\"color: #007979; font-style: italic\"># from tvm.script import relax as R</span>\n",
       "\n",
       "<span style=\"color: #AA22FF\">@I</span><span style=\"color: #AA22FF; font-weight: bold\">.</span>ir_module\n",
       "<span style=\"color: #008000; font-weight: bold\">class</span> <span style=\"color: #0000FF; font-weight: bold\">Module</span>:\n",
       "    <span style=\"color: #AA22FF\">@T</span><span style=\"color: #AA22FF; font-weight: bold\">.</span>prim_func(private<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #008000; font-weight: bold\">True</span>)\n",
       "    <span style=\"color: #008000; font-weight: bold\">def</span> <span style=\"color: #0000FF\">te_matmul</span>(x: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>Buffer((T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">1</span>), T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">128</span>)), <span style=\"color: #BA2121\">&quot;float32&quot;</span>), B: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>Buffer((T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">128</span>), T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">128</span>)), <span style=\"color: #BA2121\">&quot;float32&quot;</span>), matmul: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>Buffer((T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">1</span>), T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">128</span>)), <span style=\"color: #BA2121\">&quot;float32&quot;</span>)):\n",
       "        T<span style=\"color: #AA22FF; font-weight: bold\">.</span>func_attr({<span style=\"color: #BA2121\">&quot;tir.noalias&quot;</span>: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>bool(<span style=\"color: #008000; font-weight: bold\">True</span>)})\n",
       "        <span style=\"color: #007979; font-style: italic\"># with T.block(&quot;root&quot;):</span>\n",
       "        <span style=\"color: #008000; font-weight: bold\">for</span> i, j, k <span style=\"color: #008000; font-weight: bold\">in</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>grid(T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">1</span>), T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">128</span>), T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">128</span>)):\n",
       "            <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>block(<span style=\"color: #BA2121\">&quot;matmul&quot;</span>):\n",
       "                v_i, v_j, v_k <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>axis<span style=\"color: #AA22FF; font-weight: bold\">.</span>remap(<span style=\"color: #BA2121\">&quot;SSR&quot;</span>, [i, j, k])\n",
       "                T<span style=\"color: #AA22FF; font-weight: bold\">.</span>reads(x[v_i, v_k], B[v_k, v_j])\n",
       "                T<span style=\"color: #AA22FF; font-weight: bold\">.</span>writes(matmul[v_i, v_j])\n",
       "                <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>init():\n",
       "                    matmul[v_i, v_j] <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>float32(<span style=\"color: #008000\">0</span>)\n",
       "                matmul[v_i, v_j] <span style=\"color: #AA22FF; font-weight: bold\">=</span> matmul[v_i, v_j] <span style=\"color: #AA22FF; font-weight: bold\">+</span> x[v_i, v_k] <span style=\"color: #AA22FF; font-weight: bold\">*</span> B[v_k, v_j]\n",
       "\n",
       "    <span style=\"color: #AA22FF\">@T</span><span style=\"color: #AA22FF; font-weight: bold\">.</span>prim_func(private<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #008000; font-weight: bold\">True</span>)\n",
       "    <span style=\"color: #008000; font-weight: bold\">def</span> <span style=\"color: #0000FF\">te_relu</span>(lv: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>Buffer((T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">1</span>), T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">128</span>)), <span style=\"color: #BA2121\">&quot;float32&quot;</span>), relu: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>Buffer((T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">1</span>), T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">128</span>)), <span style=\"color: #BA2121\">&quot;float32&quot;</span>)):\n",
       "        T<span style=\"color: #AA22FF; font-weight: bold\">.</span>func_attr({<span style=\"color: #BA2121\">&quot;tir.noalias&quot;</span>: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>bool(<span style=\"color: #008000; font-weight: bold\">True</span>)})\n",
       "        <span style=\"color: #007979; font-style: italic\"># with T.block(&quot;root&quot;):</span>\n",
       "        <span style=\"color: #008000; font-weight: bold\">for</span> i0, i1 <span style=\"color: #008000; font-weight: bold\">in</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>grid(T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">1</span>), T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">128</span>)):\n",
       "            <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>block(<span style=\"color: #BA2121\">&quot;relu&quot;</span>):\n",
       "                v_i0, v_i1 <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>axis<span style=\"color: #AA22FF; font-weight: bold\">.</span>remap(<span style=\"color: #BA2121\">&quot;SS&quot;</span>, [i0, i1])\n",
       "                T<span style=\"color: #AA22FF; font-weight: bold\">.</span>reads(lv[v_i0, v_i1])\n",
       "                T<span style=\"color: #AA22FF; font-weight: bold\">.</span>writes(relu[v_i0, v_i1])\n",
       "                relu[v_i0, v_i1] <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>max(lv[v_i0, v_i1], T<span style=\"color: #AA22FF; font-weight: bold\">.</span>float32(<span style=\"color: #008000\">0</span>))\n",
       "\n",
       "    <span style=\"color: #AA22FF\">@R</span><span style=\"color: #AA22FF; font-weight: bold\">.</span>function\n",
       "    <span style=\"color: #008000; font-weight: bold\">def</span> <span style=\"color: #0000FF\">main</span>(x: R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">128</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>)) <span style=\"color: #AA22FF; font-weight: bold\">-&gt;</span> R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">128</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>):\n",
       "        cls <span style=\"color: #AA22FF; font-weight: bold\">=</span> Module\n",
       "        <span style=\"color: #008000; font-weight: bold\">with</span> R<span style=\"color: #AA22FF; font-weight: bold\">.</span>dataflow():\n",
       "            lv <span style=\"color: #AA22FF; font-weight: bold\">=</span> R<span style=\"color: #AA22FF; font-weight: bold\">.</span>call_tir(cls<span style=\"color: #AA22FF; font-weight: bold\">.</span>te_matmul, (x, metadata[<span style=\"color: #BA2121\">&quot;relax.expr.Constant&quot;</span>][<span style=\"color: #008000\">0</span>]), out_sinfo<span style=\"color: #AA22FF; font-weight: bold\">=</span>R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">128</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>))\n",
       "            lv1 <span style=\"color: #AA22FF; font-weight: bold\">=</span> R<span style=\"color: #AA22FF; font-weight: bold\">.</span>call_tir(cls<span style=\"color: #AA22FF; font-weight: bold\">.</span>te_relu, (lv,), out_sinfo<span style=\"color: #AA22FF; font-weight: bold\">=</span>R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">128</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>))\n",
       "            gv: R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">128</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #AA22FF; font-weight: bold\">=</span> lv1\n",
       "            R<span style=\"color: #AA22FF; font-weight: bold\">.</span>output(gv)\n",
       "        <span style=\"color: #008000; font-weight: bold\">return</span> lv1\n",
       "\n",
       "<span style=\"color: #007979; font-style: italic\"># Metadata omitted. Use show_meta=True in script() method to show it.</span>\n",
       "</pre></div>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def map_param(param: nn.Parameter):\n",
    "    return relax.const(param.data.cpu().numpy(), relax.TensorStructInfo(param.data.shape, \"float32\"))\n",
    "\n",
    "def fetch_attr(fx_mod, target: str):\n",
    "    \"\"\"Helper function to fetch an attr\"\"\"\n",
    "    target_atoms = target.split('.')\n",
    "    attr_itr     = fx_mod\n",
    "    for i, atom in enumerate(target_atoms):\n",
    "        if not hasattr(attr_itr, atom):\n",
    "            raise RuntimeError(f\"Node referenced nonexistant target {'.'.join(target_atoms[:i])}\")\n",
    "        attr_itr = getattr(attr_itr, atom)\n",
    "    return attr_itr\n",
    "\n",
    "def from_fx(fx_mod, input_shapes, call_function_map, call_module_map):\n",
    "    \n",
    "    input_index   = 0\n",
    "    node_map      = {}\n",
    "    named_modules = dict(fx_mod.named_modules())\n",
    "    \n",
    "    print(\"named_modules = \", named_modules)\n",
    "\n",
    "    bb = relax.BlockBuilder()\n",
    "\n",
    "    fn_inputs = []\n",
    "    fn_output = None\n",
    "    with bb.function(\"main\"):\n",
    "        with bb.dataflow():\n",
    "            for node in fx_mod.graph.nodes:    # 拓扑顺序迭代 FX 图中的节点。\n",
    "                print(\"node = \", node)\n",
    "                if node.op == \"placeholder\":\n",
    "                    # create input placeholder\n",
    "                    shape = input_shapes[input_index]\n",
    "                    input_index += 1\n",
    "                    input_var = relax.Var(\n",
    "                        node.target, relax.TensorStructInfo(shape, \"float32\")\n",
    "                    )\n",
    "                    fn_inputs.append(input_var)\n",
    "                    node_map[node] = input_var\n",
    "                elif node.op == \"get_attr\":\n",
    "                    node_map[node] = map_param(fetch_attr(fx_mod, node.target))\n",
    "                elif node.op == \"call_function\": ## pure function\n",
    "                    node_map[node] = call_function_map[node.target](bb, node_map, node)\n",
    "                elif node.op == \"call_module\":   ## 'function' with fixed parameters\n",
    "                    named_module = named_modules[node.target]\n",
    "                    print(\"type(named_module) = \", type(named_module))\n",
    "                    print(\"node.target        = \", node.target)\n",
    "                    node_map[node] = call_module_map[type(named_module)](bb, node_map, node, named_module)\n",
    "                elif node.op == \"output\":\n",
    "                    output = node_map[node.args[0]]\n",
    "                    assert fn_output is None\n",
    "                    fn_output = bb.emit_output(output)\n",
    "        # output and finalize the function\n",
    "        bb.emit_func_output(output, fn_inputs)\n",
    "    return bb.get()\n",
    "\n",
    "# 定义函数映射 \n",
    "\n",
    "def te_matmul(A: te.Tensor, B: te.Tensor) -> te.Tensor:\n",
    "    assert A.shape[1] == B.shape[0]\n",
    "    n = A.shape[0]\n",
    "    m = B.shape[1]\n",
    "    k = te.reduce_axis((0, A.shape[1]), name=\"k\")\n",
    "    return te.compute((n, m), lambda i, j: te.sum(A[i, k] * B[k, j], axis=k), name=\"matmul\")\n",
    "\n",
    "def te_relu(A: te.Tensor) -> te.Tensor:\n",
    "    return te.compute(A.shape, lambda *i: te.max(A(*i), 0), name=\"relu\")\n",
    "\n",
    "def map_matmul(bb, node_map, node: fx.Node):\n",
    "    A = node_map[node.args[0]]\n",
    "    B = node_map[node.args[1]]\n",
    "    return bb.emit_te(te_matmul, A, B)\n",
    "\n",
    "def map_relu(bb, node_map, node: fx.Node):\n",
    "    A = node_map[node.args[0]]\n",
    "    return bb.emit_te(te_relu, A)\n",
    "\n",
    "MyModule = from_fx(\n",
    "    fx_module,\n",
    "    input_shapes = [(1, 128)],\n",
    "    call_function_map = {\n",
    "      torch.matmul: map_matmul,\n",
    "      torch.relu: map_relu,\n",
    "    },\n",
    "    call_module_map={},\n",
    ")\n",
    "\n",
    "MyModule.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FASHION MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "\n",
    "test_data = torchvision.datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=torchvision.transforms.ToTensor()\n",
    ")\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=1, shuffle=True)\n",
    "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
    "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
    "\n",
    "img, label = next(iter(test_loader))\n",
    "img = img.reshape(1, 28, 28).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class: Ankle boot\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(img[0])\n",
    "plt.colorbar()\n",
    "plt.grid(False)\n",
    "plt.show()\n",
    "\n",
    "print(\"Class:\", class_names[label[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File ‘fasionmnist_mlp_params.pkl’ already there; not retrieving.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Hide outputs\n",
    "!wget -nc https://github.com/mlc-ai/web-data/raw/main/models/fasionmnist_mlp_params.pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch Prediction: Ankle boot\n"
     ]
    }
   ],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        self.linear0 = nn.Linear(784, 128, bias=True)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.linear1 = nn.Linear(128, 10, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear0(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.linear1(x)\n",
    "        return x\n",
    "\n",
    "import pickle as pkl\n",
    "\n",
    "mlp_model = MLP()\n",
    "\n",
    "mlp_params = pkl.load(open(\"fasionmnist_mlp_params.pkl\", \"rb\"))\n",
    "mlp_model.linear0.weight.data = torch.from_numpy(mlp_params[\"w0\"])\n",
    "mlp_model.linear0.bias.data   = torch.from_numpy(mlp_params[\"b0\"])\n",
    "mlp_model.linear1.weight.data = torch.from_numpy(mlp_params[\"w1\"])\n",
    "mlp_model.linear1.bias.data   = torch.from_numpy(mlp_params[\"b1\"])\n",
    "\n",
    "torch_res = mlp_model(torch.from_numpy(img.reshape(1, 784)))\n",
    "\n",
    "pred_kind = np.argmax(torch_res.detach().numpy(), axis=1)\n",
    "print(\"Torch Prediction:\", class_names[pred_kind[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "named_modules =  {'': MLP(\n",
      "  (linear0): Linear(in_features=784, out_features=128, bias=True)\n",
      "  (relu): ReLU()\n",
      "  (linear1): Linear(in_features=128, out_features=10, bias=True)\n",
      "), 'linear0': Linear(in_features=784, out_features=128, bias=True), 'relu': ReLU(), 'linear1': Linear(in_features=128, out_features=10, bias=True)}\n",
      "node =  x\n",
      "node =  linear0\n",
      "type(named_module) =  <class 'torch.nn.modules.linear.Linear'>\n",
      "node.target        =  linear0\n",
      "node =  relu\n",
      "type(named_module) =  <class 'torch.nn.modules.activation.ReLU'>\n",
      "node.target        =  relu\n",
      "node =  linear1\n",
      "type(named_module) =  <class 'torch.nn.modules.linear.Linear'>\n",
      "node.target        =  linear1\n",
      "node =  output\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"highlight\" style=\"background: \"><pre style=\"line-height: 125%;\"><span></span><span style=\"color: #007979; font-style: italic\"># from tvm.script import ir as I</span>\n",
       "<span style=\"color: #007979; font-style: italic\"># from tvm.script import tir as T</span>\n",
       "<span style=\"color: #007979; font-style: italic\"># from tvm.script import relax as R</span>\n",
       "\n",
       "<span style=\"color: #AA22FF\">@I</span><span style=\"color: #AA22FF; font-weight: bold\">.</span>ir_module\n",
       "<span style=\"color: #008000; font-weight: bold\">class</span> <span style=\"color: #0000FF; font-weight: bold\">Module</span>:\n",
       "    <span style=\"color: #AA22FF\">@T</span><span style=\"color: #AA22FF; font-weight: bold\">.</span>prim_func(private<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #008000; font-weight: bold\">True</span>)\n",
       "    <span style=\"color: #008000; font-weight: bold\">def</span> <span style=\"color: #0000FF\">add</span>(lv: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>Buffer((T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">1</span>), T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">128</span>)), <span style=\"color: #BA2121\">&quot;float32&quot;</span>), B: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>Buffer((T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">128</span>),), <span style=\"color: #BA2121\">&quot;float32&quot;</span>), T_add: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>Buffer((T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">1</span>), T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">128</span>)), <span style=\"color: #BA2121\">&quot;float32&quot;</span>)):\n",
       "        T<span style=\"color: #AA22FF; font-weight: bold\">.</span>func_attr({<span style=\"color: #BA2121\">&quot;tir.noalias&quot;</span>: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>bool(<span style=\"color: #008000; font-weight: bold\">True</span>)})\n",
       "        <span style=\"color: #007979; font-style: italic\"># with T.block(&quot;root&quot;):</span>\n",
       "        <span style=\"color: #008000; font-weight: bold\">for</span> ax0, ax1 <span style=\"color: #008000; font-weight: bold\">in</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>grid(T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">1</span>), T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">128</span>)):\n",
       "            <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>block(<span style=\"color: #BA2121\">&quot;T_add&quot;</span>):\n",
       "                v_ax0, v_ax1 <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>axis<span style=\"color: #AA22FF; font-weight: bold\">.</span>remap(<span style=\"color: #BA2121\">&quot;SS&quot;</span>, [ax0, ax1])\n",
       "                T<span style=\"color: #AA22FF; font-weight: bold\">.</span>reads(lv[v_ax0, v_ax1], B[v_ax1])\n",
       "                T<span style=\"color: #AA22FF; font-weight: bold\">.</span>writes(T_add[v_ax0, v_ax1])\n",
       "                T_add[v_ax0, v_ax1] <span style=\"color: #AA22FF; font-weight: bold\">=</span> lv[v_ax0, v_ax1] <span style=\"color: #AA22FF; font-weight: bold\">+</span> B[v_ax1]\n",
       "\n",
       "    <span style=\"color: #AA22FF\">@T</span><span style=\"color: #AA22FF; font-weight: bold\">.</span>prim_func(private<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #008000; font-weight: bold\">True</span>)\n",
       "    <span style=\"color: #008000; font-weight: bold\">def</span> <span style=\"color: #0000FF\">add1</span>(lv3: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>Buffer((T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">1</span>), T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">10</span>)), <span style=\"color: #BA2121\">&quot;float32&quot;</span>), B: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>Buffer((T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">10</span>),), <span style=\"color: #BA2121\">&quot;float32&quot;</span>), T_add: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>Buffer((T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">1</span>), T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">10</span>)), <span style=\"color: #BA2121\">&quot;float32&quot;</span>)):\n",
       "        T<span style=\"color: #AA22FF; font-weight: bold\">.</span>func_attr({<span style=\"color: #BA2121\">&quot;tir.noalias&quot;</span>: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>bool(<span style=\"color: #008000; font-weight: bold\">True</span>)})\n",
       "        <span style=\"color: #007979; font-style: italic\"># with T.block(&quot;root&quot;):</span>\n",
       "        <span style=\"color: #008000; font-weight: bold\">for</span> ax0, ax1 <span style=\"color: #008000; font-weight: bold\">in</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>grid(T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">1</span>), T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">10</span>)):\n",
       "            <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>block(<span style=\"color: #BA2121\">&quot;T_add&quot;</span>):\n",
       "                v_ax0, v_ax1 <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>axis<span style=\"color: #AA22FF; font-weight: bold\">.</span>remap(<span style=\"color: #BA2121\">&quot;SS&quot;</span>, [ax0, ax1])\n",
       "                T<span style=\"color: #AA22FF; font-weight: bold\">.</span>reads(lv3[v_ax0, v_ax1], B[v_ax1])\n",
       "                T<span style=\"color: #AA22FF; font-weight: bold\">.</span>writes(T_add[v_ax0, v_ax1])\n",
       "                T_add[v_ax0, v_ax1] <span style=\"color: #AA22FF; font-weight: bold\">=</span> lv3[v_ax0, v_ax1] <span style=\"color: #AA22FF; font-weight: bold\">+</span> B[v_ax1]\n",
       "\n",
       "    <span style=\"color: #AA22FF\">@T</span><span style=\"color: #AA22FF; font-weight: bold\">.</span>prim_func(private<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #008000; font-weight: bold\">True</span>)\n",
       "    <span style=\"color: #008000; font-weight: bold\">def</span> <span style=\"color: #0000FF\">dense</span>(x: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>Buffer((T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">1</span>), T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">784</span>)), <span style=\"color: #BA2121\">&quot;float32&quot;</span>), B: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>Buffer((T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">128</span>), T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">784</span>)), <span style=\"color: #BA2121\">&quot;float32&quot;</span>), T_matmul_NT: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>Buffer((T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">1</span>), T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">128</span>)), <span style=\"color: #BA2121\">&quot;float32&quot;</span>)):\n",
       "        T<span style=\"color: #AA22FF; font-weight: bold\">.</span>func_attr({<span style=\"color: #BA2121\">&quot;layout_free_buffers&quot;</span>: [<span style=\"color: #008000\">1</span>], <span style=\"color: #BA2121\">&quot;tir.noalias&quot;</span>: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>bool(<span style=\"color: #008000; font-weight: bold\">True</span>)})\n",
       "        <span style=\"color: #007979; font-style: italic\"># with T.block(&quot;root&quot;):</span>\n",
       "        <span style=\"color: #008000; font-weight: bold\">for</span> i0, i1, k <span style=\"color: #008000; font-weight: bold\">in</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>grid(T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">1</span>), T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">128</span>), T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">784</span>)):\n",
       "            <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>block(<span style=\"color: #BA2121\">&quot;T_matmul_NT&quot;</span>):\n",
       "                v_i0, v_i1, v_k <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>axis<span style=\"color: #AA22FF; font-weight: bold\">.</span>remap(<span style=\"color: #BA2121\">&quot;SSR&quot;</span>, [i0, i1, k])\n",
       "                T<span style=\"color: #AA22FF; font-weight: bold\">.</span>reads(x[v_i0, v_k], B[v_i1, v_k])\n",
       "                T<span style=\"color: #AA22FF; font-weight: bold\">.</span>writes(T_matmul_NT[v_i0, v_i1])\n",
       "                <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>init():\n",
       "                    T_matmul_NT[v_i0, v_i1] <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>float32(<span style=\"color: #008000\">0</span>)\n",
       "                T_matmul_NT[v_i0, v_i1] <span style=\"color: #AA22FF; font-weight: bold\">=</span> T_matmul_NT[v_i0, v_i1] <span style=\"color: #AA22FF; font-weight: bold\">+</span> x[v_i0, v_k] <span style=\"color: #AA22FF; font-weight: bold\">*</span> B[v_i1, v_k]\n",
       "\n",
       "    <span style=\"color: #AA22FF\">@T</span><span style=\"color: #AA22FF; font-weight: bold\">.</span>prim_func(private<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #008000; font-weight: bold\">True</span>)\n",
       "    <span style=\"color: #008000; font-weight: bold\">def</span> <span style=\"color: #0000FF\">dense1</span>(lv2: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>Buffer((T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">1</span>), T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">128</span>)), <span style=\"color: #BA2121\">&quot;float32&quot;</span>), B: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>Buffer((T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">10</span>), T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">128</span>)), <span style=\"color: #BA2121\">&quot;float32&quot;</span>), T_matmul_NT: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>Buffer((T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">1</span>), T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">10</span>)), <span style=\"color: #BA2121\">&quot;float32&quot;</span>)):\n",
       "        T<span style=\"color: #AA22FF; font-weight: bold\">.</span>func_attr({<span style=\"color: #BA2121\">&quot;layout_free_buffers&quot;</span>: [<span style=\"color: #008000\">1</span>], <span style=\"color: #BA2121\">&quot;tir.noalias&quot;</span>: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>bool(<span style=\"color: #008000; font-weight: bold\">True</span>)})\n",
       "        <span style=\"color: #007979; font-style: italic\"># with T.block(&quot;root&quot;):</span>\n",
       "        <span style=\"color: #008000; font-weight: bold\">for</span> i0, i1, k <span style=\"color: #008000; font-weight: bold\">in</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>grid(T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">1</span>), T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">10</span>), T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">128</span>)):\n",
       "            <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>block(<span style=\"color: #BA2121\">&quot;T_matmul_NT&quot;</span>):\n",
       "                v_i0, v_i1, v_k <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>axis<span style=\"color: #AA22FF; font-weight: bold\">.</span>remap(<span style=\"color: #BA2121\">&quot;SSR&quot;</span>, [i0, i1, k])\n",
       "                T<span style=\"color: #AA22FF; font-weight: bold\">.</span>reads(lv2[v_i0, v_k], B[v_i1, v_k])\n",
       "                T<span style=\"color: #AA22FF; font-weight: bold\">.</span>writes(T_matmul_NT[v_i0, v_i1])\n",
       "                <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>init():\n",
       "                    T_matmul_NT[v_i0, v_i1] <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>float32(<span style=\"color: #008000\">0</span>)\n",
       "                T_matmul_NT[v_i0, v_i1] <span style=\"color: #AA22FF; font-weight: bold\">=</span> T_matmul_NT[v_i0, v_i1] <span style=\"color: #AA22FF; font-weight: bold\">+</span> lv2[v_i0, v_k] <span style=\"color: #AA22FF; font-weight: bold\">*</span> B[v_i1, v_k]\n",
       "\n",
       "    <span style=\"color: #AA22FF\">@T</span><span style=\"color: #AA22FF; font-weight: bold\">.</span>prim_func(private<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #008000; font-weight: bold\">True</span>)\n",
       "    <span style=\"color: #008000; font-weight: bold\">def</span> <span style=\"color: #0000FF\">te_relu</span>(lv1: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>Buffer((T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">1</span>), T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">128</span>)), <span style=\"color: #BA2121\">&quot;float32&quot;</span>), relu: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>Buffer((T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">1</span>), T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">128</span>)), <span style=\"color: #BA2121\">&quot;float32&quot;</span>)):\n",
       "        T<span style=\"color: #AA22FF; font-weight: bold\">.</span>func_attr({<span style=\"color: #BA2121\">&quot;tir.noalias&quot;</span>: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>bool(<span style=\"color: #008000; font-weight: bold\">True</span>)})\n",
       "        <span style=\"color: #007979; font-style: italic\"># with T.block(&quot;root&quot;):</span>\n",
       "        <span style=\"color: #008000; font-weight: bold\">for</span> i0, i1 <span style=\"color: #008000; font-weight: bold\">in</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>grid(T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">1</span>), T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">128</span>)):\n",
       "            <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>block(<span style=\"color: #BA2121\">&quot;relu&quot;</span>):\n",
       "                v_i0, v_i1 <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>axis<span style=\"color: #AA22FF; font-weight: bold\">.</span>remap(<span style=\"color: #BA2121\">&quot;SS&quot;</span>, [i0, i1])\n",
       "                T<span style=\"color: #AA22FF; font-weight: bold\">.</span>reads(lv1[v_i0, v_i1])\n",
       "                T<span style=\"color: #AA22FF; font-weight: bold\">.</span>writes(relu[v_i0, v_i1])\n",
       "                relu[v_i0, v_i1] <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>max(lv1[v_i0, v_i1], T<span style=\"color: #AA22FF; font-weight: bold\">.</span>float32(<span style=\"color: #008000\">0</span>))\n",
       "\n",
       "    <span style=\"color: #AA22FF\">@R</span><span style=\"color: #AA22FF; font-weight: bold\">.</span>function\n",
       "    <span style=\"color: #008000; font-weight: bold\">def</span> <span style=\"color: #0000FF\">main</span>(x: R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">784</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>)) <span style=\"color: #AA22FF; font-weight: bold\">-&gt;</span> R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">10</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>):\n",
       "        cls <span style=\"color: #AA22FF; font-weight: bold\">=</span> Module\n",
       "        <span style=\"color: #008000; font-weight: bold\">with</span> R<span style=\"color: #AA22FF; font-weight: bold\">.</span>dataflow():\n",
       "            lv <span style=\"color: #AA22FF; font-weight: bold\">=</span> R<span style=\"color: #AA22FF; font-weight: bold\">.</span>call_tir(cls<span style=\"color: #AA22FF; font-weight: bold\">.</span>dense, (x, metadata[<span style=\"color: #BA2121\">&quot;relax.expr.Constant&quot;</span>][<span style=\"color: #008000\">0</span>]), out_sinfo<span style=\"color: #AA22FF; font-weight: bold\">=</span>R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">128</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>))\n",
       "            lv1 <span style=\"color: #AA22FF; font-weight: bold\">=</span> R<span style=\"color: #AA22FF; font-weight: bold\">.</span>call_tir(cls<span style=\"color: #AA22FF; font-weight: bold\">.</span>add, (lv, metadata[<span style=\"color: #BA2121\">&quot;relax.expr.Constant&quot;</span>][<span style=\"color: #008000\">1</span>]), out_sinfo<span style=\"color: #AA22FF; font-weight: bold\">=</span>R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">128</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>))\n",
       "            lv2 <span style=\"color: #AA22FF; font-weight: bold\">=</span> R<span style=\"color: #AA22FF; font-weight: bold\">.</span>call_tir(cls<span style=\"color: #AA22FF; font-weight: bold\">.</span>te_relu, (lv1,), out_sinfo<span style=\"color: #AA22FF; font-weight: bold\">=</span>R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">128</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>))\n",
       "            lv3 <span style=\"color: #AA22FF; font-weight: bold\">=</span> R<span style=\"color: #AA22FF; font-weight: bold\">.</span>call_tir(cls<span style=\"color: #AA22FF; font-weight: bold\">.</span>dense1, (lv2, metadata[<span style=\"color: #BA2121\">&quot;relax.expr.Constant&quot;</span>][<span style=\"color: #008000\">2</span>]), out_sinfo<span style=\"color: #AA22FF; font-weight: bold\">=</span>R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">10</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>))\n",
       "            lv4 <span style=\"color: #AA22FF; font-weight: bold\">=</span> R<span style=\"color: #AA22FF; font-weight: bold\">.</span>call_tir(cls<span style=\"color: #AA22FF; font-weight: bold\">.</span>add1, (lv3, metadata[<span style=\"color: #BA2121\">&quot;relax.expr.Constant&quot;</span>][<span style=\"color: #008000\">3</span>]), out_sinfo<span style=\"color: #AA22FF; font-weight: bold\">=</span>R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">10</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>))\n",
       "            gv: R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">10</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #AA22FF; font-weight: bold\">=</span> lv4\n",
       "            R<span style=\"color: #AA22FF; font-weight: bold\">.</span>output(gv)\n",
       "        <span style=\"color: #008000; font-weight: bold\">return</span> lv4\n",
       "\n",
       "<span style=\"color: #007979; font-style: italic\"># Metadata omitted. Use show_meta=True in script() method to show it.</span>\n",
       "</pre></div>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tvm import topi\n",
    "\n",
    "\n",
    "def map_nn_linear(bb, node_map, node, nn_mod):\n",
    "    x = node_map[node.args[0]]\n",
    "    w = map_param(nn_mod.weight)\n",
    "    if nn_mod.bias is not None:\n",
    "        b = map_param(nn_mod.bias)\n",
    "    y = bb.emit_te(topi.nn.dense, x, w)\n",
    "    return bb.emit_te(topi.add, y, b)\n",
    "\n",
    "def map_nn_relu(bb, node_map, node, nn_mod):\n",
    "    return map_relu(bb, node_map, node)\n",
    "\n",
    "\n",
    "MLPModule = from_fx(\n",
    "    fx.symbolic_trace(mlp_model),\n",
    "    input_shapes = [(1, 784)],\n",
    "    call_function_map={\n",
    "    },  # 注意这里无 function ! \n",
    "    call_module_map={\n",
    "        torch.nn.Linear: map_nn_linear,\n",
    "        torch.nn.ReLU: map_nn_relu,\n",
    "    },  # 所有的都是 module ! \n",
    ")\n",
    "\n",
    "MLPModule.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 备注：翻译成高层算子"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "named_modules =  {'': MLP(\n",
      "  (linear0): Linear(in_features=784, out_features=128, bias=True)\n",
      "  (relu): ReLU()\n",
      "  (linear1): Linear(in_features=128, out_features=10, bias=True)\n",
      "), 'linear0': Linear(in_features=784, out_features=128, bias=True), 'relu': ReLU(), 'linear1': Linear(in_features=128, out_features=10, bias=True)}\n",
      "node =  x\n",
      "node =  linear0\n",
      "type(named_module) =  <class 'torch.nn.modules.linear.Linear'>\n",
      "node.target        =  linear0\n",
      "node =  relu\n",
      "type(named_module) =  <class 'torch.nn.modules.activation.ReLU'>\n",
      "node.target        =  relu\n",
      "node =  linear1\n",
      "type(named_module) =  <class 'torch.nn.modules.linear.Linear'>\n",
      "node.target        =  linear1\n",
      "node =  output\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"highlight\" style=\"background: \"><pre style=\"line-height: 125%;\"><span></span><span style=\"color: #007979; font-style: italic\"># from tvm.script import ir as I</span>\n",
       "<span style=\"color: #007979; font-style: italic\"># from tvm.script import relax as R</span>\n",
       "\n",
       "<span style=\"color: #AA22FF\">@I</span><span style=\"color: #AA22FF; font-weight: bold\">.</span>ir_module\n",
       "<span style=\"color: #008000; font-weight: bold\">class</span> <span style=\"color: #0000FF; font-weight: bold\">Module</span>:\n",
       "    <span style=\"color: #AA22FF\">@R</span><span style=\"color: #AA22FF; font-weight: bold\">.</span>function\n",
       "    <span style=\"color: #008000; font-weight: bold\">def</span> <span style=\"color: #0000FF\">main</span>(x: R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">784</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>)) <span style=\"color: #AA22FF; font-weight: bold\">-&gt;</span> R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">10</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>):\n",
       "        <span style=\"color: #008000; font-weight: bold\">with</span> R<span style=\"color: #AA22FF; font-weight: bold\">.</span>dataflow():\n",
       "            lv: R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">784</span>, <span style=\"color: #008000\">128</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #AA22FF; font-weight: bold\">=</span> R<span style=\"color: #AA22FF; font-weight: bold\">.</span>permute_dims(metadata[<span style=\"color: #BA2121\">&quot;relax.expr.Constant&quot;</span>][<span style=\"color: #008000\">0</span>], axes<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #008000; font-weight: bold\">None</span>)\n",
       "            lv1: R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">128</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #AA22FF; font-weight: bold\">=</span> R<span style=\"color: #AA22FF; font-weight: bold\">.</span>matmul(x, lv, out_dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;void&quot;</span>)\n",
       "            lv2: R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">128</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #AA22FF; font-weight: bold\">=</span> R<span style=\"color: #AA22FF; font-weight: bold\">.</span>add(lv1, metadata[<span style=\"color: #BA2121\">&quot;relax.expr.Constant&quot;</span>][<span style=\"color: #008000\">1</span>])\n",
       "            lv3: R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">128</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #AA22FF; font-weight: bold\">=</span> R<span style=\"color: #AA22FF; font-weight: bold\">.</span>nn<span style=\"color: #AA22FF; font-weight: bold\">.</span>relu(lv2)\n",
       "            lv4: R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">128</span>, <span style=\"color: #008000\">10</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #AA22FF; font-weight: bold\">=</span> R<span style=\"color: #AA22FF; font-weight: bold\">.</span>permute_dims(metadata[<span style=\"color: #BA2121\">&quot;relax.expr.Constant&quot;</span>][<span style=\"color: #008000\">2</span>], axes<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #008000; font-weight: bold\">None</span>)\n",
       "            lv5: R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">10</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #AA22FF; font-weight: bold\">=</span> R<span style=\"color: #AA22FF; font-weight: bold\">.</span>matmul(lv3, lv4, out_dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;void&quot;</span>)\n",
       "            lv6: R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">10</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #AA22FF; font-weight: bold\">=</span> R<span style=\"color: #AA22FF; font-weight: bold\">.</span>add(lv5, metadata[<span style=\"color: #BA2121\">&quot;relax.expr.Constant&quot;</span>][<span style=\"color: #008000\">3</span>])\n",
       "            gv: R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">10</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #AA22FF; font-weight: bold\">=</span> lv6\n",
       "            R<span style=\"color: #AA22FF; font-weight: bold\">.</span>output(gv)\n",
       "        <span style=\"color: #008000; font-weight: bold\">return</span> lv6\n",
       "\n",
       "<span style=\"color: #007979; font-style: italic\"># Metadata omitted. Use show_meta=True in script() method to show it.</span>\n",
       "</pre></div>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def map_nn_relu_op(bb, node_map, node, nn_mod):\n",
    "    A = node_map[node.args[0]]\n",
    "    return bb.emit(relax.op.nn.relu(A))\n",
    "\n",
    "def map_nn_linear_op(bb, node_map, node, nn_mod):\n",
    "    x = node_map[node.args[0]]\n",
    "    w = map_param(nn_mod.weight)\n",
    "    b = map_param(nn_mod.bias)\n",
    "    return bb.emit(relax.op.linear(x, w, b))\n",
    "\n",
    "MLPModuleHighLevel = from_fx(\n",
    "    fx.symbolic_trace(mlp_model),\n",
    "    input_shapes = [(1, 784)],\n",
    "    call_function_map={\n",
    "    },\n",
    "    call_module_map={\n",
    "        torch.nn.Linear: map_nn_linear_op,\n",
    "        torch.nn.ReLU: map_nn_relu_op,\n",
    "    },\n",
    ")\n",
    "\n",
    "MLPModuleHighLevel.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyscf_isdf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
