{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tvm\n",
    "from tvm import relax\n",
    "from tvm.ir.module import IRModule\n",
    "from tvm.script import relax as R\n",
    "from tvm.script import tir as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pycuda.driver as cuda\n",
    "\n",
    "# Initialize the CUDA driver\n",
    "cuda.init()\n",
    "\n",
    "# Get the first CUDA device\n",
    "device = cuda.Device(0)\n",
    "\n",
    "# Print the device name (version and model)\n",
    "print('GPU version and model:', device.name())\n",
    "\n",
    "compute_capability = device.compute_capability()\n",
    "print('GPU compute capability:', compute_capability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tvm.script.ir_module\n",
    "class MyModuleVecAdd:\n",
    "    @T.prim_func\n",
    "    def main(A: T.Buffer((1024,), \"float32\"),\n",
    "             B: T.Buffer((1024,), \"float32\"),\n",
    "             C: T.Buffer((1024,), \"float32\")) -> None:\n",
    "        T.func_attr({\"global_symbol\": \"main\", \"tir.noalias\": True})\n",
    "        for i in T.grid(1024):\n",
    "            with T.block(\"C\"):\n",
    "                vi = T.axis.remap(\"S\", [i])\n",
    "                C[vi] = A[vi] + B[vi]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sch     = tvm.tir.Schedule(MyModuleVecAdd)\n",
    "block_C = sch.get_block(\"C\")\n",
    "i,      = sch.get_loops(block=block_C)\n",
    "i0, i1  = sch.split(i, [None, 128])\n",
    "sch.mod.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sch.bind(i0, \"blockIdx.x\")\n",
    "sch.bind(i1, \"threadIdx.x\")\n",
    "sch.mod.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### GPU 构造 ###\n",
    "\n",
    "rt_mod = tvm.build(sch.mod, target=\"cuda -arch=sm_89\")\n",
    "\n",
    "A_np = np.random.uniform(size=(1024,)).astype(\"float32\")\n",
    "B_np = np.random.uniform(size=(1024,)).astype(\"float32\")\n",
    "A_nd = tvm.nd.array(A_np, tvm.cuda(0))\n",
    "B_nd = tvm.nd.array(B_np, tvm.cuda(0))\n",
    "C_nd = tvm.nd.array(np.zeros((1024,), dtype=\"float32\"), tvm.cuda(0))\n",
    "\n",
    "rt_mod[\"main\"](A_nd, B_nd, C_nd)\n",
    "print(A_nd)\n",
    "print(B_nd)\n",
    "print(C_nd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 窗口求和 \n",
    "\n",
    "@tvm.script.ir_module\n",
    "class MyModuleWindowSum:\n",
    "    @T.prim_func\n",
    "    def main(A: T.Buffer((1027,), \"float32\"),\n",
    "             B: T.Buffer((1024,), \"float32\")) -> None:\n",
    "        T.func_attr({\"global_symbol\": \"main\", \"tir.noalias\": True})\n",
    "        for i in T.grid(1024):\n",
    "            with T.block(\"C\"):\n",
    "                vi = T.axis.remap(\"S\", [i])\n",
    "                B[vi] = A[vi] + A[vi + 1] + A[vi + 2]\n",
    "\n",
    "sch = tvm.tir.Schedule(MyModuleWindowSum)\n",
    "nthread = 128\n",
    "block_C = sch.get_block(\"C\")\n",
    "i,  = sch.get_loops(block=block_C)\n",
    "i0, i1 = sch.split(i, [None, nthread])\n",
    "sch.bind(i0, \"blockIdx.x\")\n",
    "sch.bind(i1, \"threadIdx.x\")\n",
    "sch.mod.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 数据共用\n",
    "\n",
    "### 因为内存是跨线程共享的，所以我们需要重新拆分循环并将获取过程的内部迭代器绑定到线程索引上。这种技术称为 cooperative fetching，其中多个线程一起工作以将数据带到共享内存中。\n",
    "\n",
    "A_shared = sch.cache_read(block_C, read_buffer_index=0, storage_scope=\"shared\")\n",
    "sch.compute_at(A_shared, i1)\n",
    "sch.mod.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sch.get_loops(A_shared)[-1]\n",
    "ax0, ax1 = sch.split(ax, [None, nthread])\n",
    "sch.bind(ax1, \"threadIdx.x\")\n",
    "sch.mod.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rt_mod = tvm.build(sch.mod, target=\"cuda\")\n",
    "print(rt_mod.imported_modules[0].get_source())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 矩阵相乘"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tvm.script.ir_module\n",
    "class MyModuleMatmul:\n",
    "    @T.prim_func\n",
    "    def main(A: T.Buffer((1024, 1024), \"float32\"),\n",
    "             B: T.Buffer((1024, 1024), \"float32\"),\n",
    "             C: T.Buffer((1024, 1024), \"float32\")) -> None:\n",
    "        T.func_attr({\"global_symbol\": \"main\", \"tir.noalias\": True})\n",
    "        for i, j, k in T.grid(1024, 1024, 1024):\n",
    "            with T.block(\"C\"):\n",
    "                vi, vj, vk = T.axis.remap(\"SSR\", [i, j, k])\n",
    "                with T.init():\n",
    "                    C[vi, vj] = 0.0\n",
    "                C[vi, vj] = C[vi, vj] + A[vi, vk] * B[vk, vj]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def blocking(sch,\n",
    "             tile_local_y,\n",
    "             tile_local_x,\n",
    "             tile_block_y,\n",
    "             tile_block_x,\n",
    "             tile_k):\n",
    "    block_C = sch.get_block(\"C\")\n",
    "    C_local = sch.cache_write(block_C, 0, \"local\")\n",
    "\n",
    "    i, j, k = sch.get_loops(block=block_C)\n",
    "\n",
    "    i0, i1, i2 = sch.split(loop=i, factors=[None, tile_block_y, tile_local_y])\n",
    "    j0, j1, j2 = sch.split(loop=j, factors=[None, tile_block_x, tile_local_x])\n",
    "    k0, k1 = sch.split(loop=k, factors=[None, tile_k])\n",
    "    sch.unroll(k1)\n",
    "    sch.reorder(i0, j0, i1, j1, k0, k1, i2, j2)\n",
    "    sch.reverse_compute_at(C_local, j1)  ## 问题是 C_local 在每个 thread 内部是连续的吗 ? \n",
    "\n",
    "    sch.bind(i0, \"blockIdx.y\")\n",
    "    sch.bind(j0, \"blockIdx.x\")\n",
    "\n",
    "    sch.bind(i1, \"threadIdx.y\")\n",
    "    sch.bind(j1, \"threadIdx.x\")\n",
    "    sch.decompose_reduction(block_C, k0)\n",
    "\n",
    "    return sch\n",
    "\n",
    "sch = tvm.tir.Schedule(MyModuleMatmul)\n",
    "sch = blocking(sch, 8, 8, 8, 8, 4)\n",
    "sch.mod.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rt_mod = tvm.build(sch.mod, target=\"cuda\")\n",
    "dev = tvm.cuda(0)\n",
    "A_np = np.random.uniform(size=(1024, 1024)).astype(\"float32\")\n",
    "B_np = np.random.uniform(size=(1024, 1024)).astype(\"float32\")\n",
    "A_nd = tvm.nd.array(A_np, dev)\n",
    "B_nd = tvm.nd.array(B_np, dev)\n",
    "C_nd = tvm.nd.array(np.zeros((1024, 1024), dtype=\"float32\"), dev)\n",
    "\n",
    "num_flop = 2 * 1024 * 1024 * 1024\n",
    "evaluator = rt_mod.time_evaluator(\"main\", dev, number=10)\n",
    "\n",
    "print(\"GEMM-Blocking: %f GFLOPS\" % (num_flop / evaluator(A_nd, B_nd, C_nd).mean / 1e9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cache_read_and_coop_fetch(sch, block, nthread, read_idx, read_loc):\n",
    "    read_cache = sch.cache_read(block=block, read_buffer_index=read_idx, storage_scope=\"shared\")\n",
    "    sch.compute_at(block=read_cache, loop=read_loc)\n",
    "    # vectorized cooperative fetch\n",
    "    inner0, inner1 = sch.get_loops(block=read_cache)[-2:]\n",
    "    inner = sch.fuse(inner0, inner1)\n",
    "    _, tx, vec = sch.split(loop=inner, factors=[None, nthread, 4])\n",
    "    sch.vectorize(vec)\n",
    "    sch.bind(tx, \"threadIdx.x\")\n",
    "\n",
    "\n",
    "def blocking_with_shared(\n",
    "    sch,\n",
    "    tile_local_y,\n",
    "    tile_local_x,\n",
    "    tile_block_y,\n",
    "    tile_block_x,\n",
    "    tile_k):\n",
    "    block_C = sch.get_block(\"C\")\n",
    "    C_local = sch.cache_write(block_C, 0, \"local\")\n",
    "\n",
    "    i, j, k = sch.get_loops(block=block_C)\n",
    "\n",
    "    i0, i1, i2 = sch.split(loop=i, factors=[None, tile_block_y, tile_local_y])\n",
    "    j0, j1, j2 = sch.split(loop=j, factors=[None, tile_block_x, tile_local_x])\n",
    "    k0, k1 = sch.split(loop=k, factors=[None, tile_k])\n",
    "\n",
    "    sch.reorder(i0, j0, i1, j1, k0, k1, i2, j2)\n",
    "    sch.reverse_compute_at(C_local, j1)\n",
    "\n",
    "    sch.bind(i0, \"blockIdx.y\")\n",
    "    sch.bind(j0, \"blockIdx.x\")\n",
    "\n",
    "    tx = sch.fuse(i1, j1)\n",
    "    sch.bind(tx, \"threadIdx.x\")\n",
    "    nthread = tile_block_y * tile_block_x\n",
    "    cache_read_and_coop_fetch(sch, block_C, nthread, 0, k0)\n",
    "    cache_read_and_coop_fetch(sch, block_C, nthread, 1, k0)\n",
    "    sch.decompose_reduction(block_C, k0)\n",
    "\n",
    "    return sch\n",
    "\n",
    "sch = tvm.tir.Schedule(MyModuleMatmul)\n",
    "sch = blocking_with_shared(sch, 8, 8, 8, 8, 8)\n",
    "sch.mod.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rt_mod = tvm.build(sch.mod, target=\"cuda\")\n",
    "dev = tvm.cuda(0)\n",
    "evaluator = rt_mod.time_evaluator(\"main\", dev, number=10)\n",
    "\n",
    "print(\"GEMM-Blocking: %f GFLOPS\" % (num_flop / evaluator(A_nd, B_nd, C_nd).mean / 1e9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tvm import meta_schedule as ms\n",
    "\n",
    "database = ms.tune_tir(\n",
    "    mod=MyModuleMatmul,\n",
    "    target=tvm.target.cuda(model=\"4050\",arch=\"sm_89\"),\n",
    "    max_trials_global=64,\n",
    "    num_trials_per_iter=64,\n",
    "    work_dir=\"./tune_tmp\",\n",
    "    # task_name=\"main\"\n",
    ")\n",
    "sch = ms.tir_integration.compile_tir(database, MyModuleMatmul, \"nvidia/RTX-4050\")\n",
    "sch.mod.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyscf_isdf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
